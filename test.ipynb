{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    " \n",
    "[-] Remove the primary columns from evaluation  \n",
    "[-] Make to top number variable based on inicial class input  \n",
    "[-] Insert a button the follow the page to return to top  \n",
    "[ ] Reagenge report render to a unique function/class  \n",
    "[ ] Test using a continuos target  \n",
    "[ ] Test using categorical variables  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils import SandEDA\n",
    "\n",
    "df = pd.read_csv(\"./database/UCI_Credit_Card.csv\")\n",
    "\n",
    "# Define the specific dates range\n",
    "start_date = \"2023-06-01\"\n",
    "end_date = \"2023-09-30\"\n",
    "\n",
    "safra = []\n",
    "# Generate a random date within the specified range\n",
    "for i in range(df.shape[0]):\n",
    "    safra.append(\n",
    "        pd.to_datetime(\n",
    "            np.random.choice(pd.date_range(start=start_date, end=end_date))\n",
    "        ).strftime(\"%Y%m\")\n",
    "    )\n",
    "\n",
    "df[\"safra\"] = safra\n",
    "df.rename(\n",
    "    columns={\n",
    "        \"default.payment.next.month\": \"default\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from jinja2 import Template\n",
    "\n",
    "\n",
    "from utils import SandEDA\n",
    "\n",
    "df = pd.read_csv(\"./database/Playground_Churn_Bank.csv\")\n",
    "\n",
    "del df['id']\n",
    "\n",
    "# Define the specific dates range\n",
    "start_date = \"2023-06-01\"\n",
    "end_date = \"2023-09-30\"\n",
    "\n",
    "safra = []\n",
    "# Generate a random date within the specified range\n",
    "for i in range(df.shape[0]):\n",
    "    safra.append(\n",
    "        pd.to_datetime(\n",
    "            np.random.choice(pd.date_range(start=start_date, end=end_date))\n",
    "        ).strftime(\"%Y%m\")\n",
    "    )\n",
    "\n",
    "df[\"safra\"] = safra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandeda = SandEDA(df, \"Exited\", \"safra\", \"CustomerId\")\n",
    "sandeda.report('CHURN_0_0_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.7202\n",
      "Random Forest Accuracy: 0.8137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/Projects/janestreet/.venv/lib/python3.12/site-packages/xgboost/core.py:158: UserWarning: [21:22:07] WARNING: /workspace/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.8167\n",
      "Logistic Regression Accuracy: 0.8092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/Projects/janestreet/.venv/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifie[\"descriptive_statistics\"]\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the features and target\n",
    "X = df.drop(columns=['default', 'ID', 'safra'])\n",
    "y = df['default']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the classifiers\n",
    "classifiers = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"{name} Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OVERVxIEW\n",
    "\n",
    "sandeda = SandEDA(df, \"Exited\", \"safra\", \"CustomerId\")\n",
    "res_general = sandeda.calc_general()\n",
    "\n",
    "overview_target_metric_time = res_general[\"target_general\"][\"target_metric_time\"]\n",
    "\n",
    "# Create the bar plot using Altair\n",
    "chart = (\n",
    "    alt.Chart(\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"Date\": list(overview_target_metric_time.keys()),\n",
    "                \"%\": list(overview_target_metric_time.values()),\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    .mark_bar()\n",
    "    .encode(x=alt.X(\"Date\", sort=\"ascending\"), y=alt.Y(\"%\"))\n",
    "    .properties(width=100, height=100)\n",
    "    .configure_axis(labelAngle=45)\n",
    "    .configure_title(fontSize=10)\n",
    ")\n",
    "\n",
    "\n",
    "overview_tab_general = pd.DataFrame(\n",
    "                        list(res_general[\"dataset_general\"].items()),\n",
    "                        columns=[\"description\", \"value\"],\n",
    "                        index=None,\n",
    "                        ).to_html(index=False, border=0)\n",
    "\n",
    "overview_tab_full = pd.DataFrame(res_general[\"missing_zero\"]).to_html(\n",
    "                        index=False, \n",
    "                        border=0)\n",
    "\n",
    "overview_target_name = res_general[\"target_general\"][\"target_name\"]\n",
    "\n",
    "overview_target_metric = (\n",
    "    res_general[\"target_general\"][\"number_of_one\"]\n",
    "    / (\n",
    "        res_general[\"target_general\"][\"number_of_zero\"]\n",
    "        + res_general[\"target_general\"][\"number_of_one\"]\n",
    "    )\n",
    ") * 100\n",
    "\n",
    "\n",
    "overview_tgt_graph_json = chart.to_json()\n",
    "\n",
    "\n",
    "### VARIABLES\n",
    "\n",
    "iv_, mi_ = sandeda.promising_features()\n",
    "psi_, ks_ = sandeda.variables_estability()\n",
    "miss_, zero_ = sandeda.variables_fillment()\n",
    "\n",
    "var_tab_ks = pd.DataFrame({\n",
    "    'Variable': [var for var, _ in ks_],\n",
    "    'Max KS': [max(value.values()) for _, value in ks_]\n",
    "}).to_html(index=False, border=0)\n",
    "\n",
    "var_tab_psi = pd.DataFrame({\n",
    "    'Variable': [var for var, _ in psi_],\n",
    "    'Max PSI': [max(value.values()) for _, value in psi_]\n",
    "}).to_html(index=False, border=0)\n",
    "\n",
    "\n",
    "var_tab_iv = pd.DataFrame({\n",
    "    'Variable': [var for var, _ in iv_],\n",
    "    'IV': [value for _, value in iv_]\n",
    "}).to_html(index=False, border=0)\n",
    "\n",
    "var_tab_mi = pd.DataFrame({\n",
    "    'Variable': [var for var, _ in mi_],\n",
    "    'MI': [value for _, value in mi_]\n",
    "}).to_html(index=False, border=0)\n",
    "\n",
    "\n",
    "var_tab_miss = pd.DataFrame(miss_).to_html(index=False, border=0)\n",
    "var_tab_zero = pd.DataFrame(zero_).to_html(index=False, border=0)\n",
    "\n",
    "\n",
    "### VARIABLES ESPECIFICS\n",
    "\n",
    "variables_espec = sandeda.variables_espec()\n",
    "\n",
    "variables_espec_time = sandeda.variables_espec_time()\n",
    "\n",
    "\n",
    "var_espec_content = {}\n",
    "\n",
    "vars_keys = variables_espec.keys() - {\"Exited\", \"safra\", \"CustomerId\"}\n",
    "\n",
    "for var_espec in vars_keys:\n",
    "\n",
    "    hist_var = variables_espec[var_espec]['histogram']\n",
    "\n",
    "    decil_var = variables_espec[var_espec]['decil']\n",
    "\n",
    "    del variables_espec[var_espec]['histogram'], variables_espec[var_espec]['decil']\n",
    "\n",
    "    var_spec_tab_desc = pd.DataFrame(\n",
    "        {\n",
    "            \"description\": variables_espec[var_espec][\"descriptive_statistics\"].keys(),\n",
    "            \"value\": variables_espec[var_espec][\"descriptive_statistics\"].values(),\n",
    "        }).to_html(index=False, border=0)\n",
    "\n",
    "\n",
    "    var_spec_tab_quant = pd.DataFrame(\n",
    "        {\n",
    "            \"description\": variables_espec[var_espec][\"quantile_statistics\"].keys(),\n",
    "            \"value\": variables_espec[var_espec][\"quantile_statistics\"].values(),\n",
    "        }).to_html(index=False, border=0)\n",
    "\n",
    "    if variables_espec[var_espec][\"descriptive_statistics\"][\"number_of_unique_values\"] <= 50 or variables_espec[var_espec][\"descriptive_statistics\"]['variable_type'] not in [\"int64\", \"float64\"]:\n",
    "        # Convert the histogram data to a dataframe\n",
    "        hist_data = pd.DataFrame({\n",
    "            'Interval': list(hist_var.keys()),\n",
    "            'Count': list(hist_var.values())\n",
    "        })\n",
    "    else:\n",
    "        hist_data = pd.DataFrame({\n",
    "            'Interval': [f\"{interval.left:.1f}\" for interval in hist_var.keys()],\n",
    "            'Count': list(hist_var.values())\n",
    "        })\n",
    "\n",
    "    # Create the bar plot using Altair\n",
    "    hist_chart = alt.Chart(hist_data).mark_bar().encode(\n",
    "        x=alt.X('Interval', title='', sort=None, axis=alt.Axis(labels=True, labelOverlap=True, labelFontSize=9)),\n",
    "        y=alt.Y('Count', title='Qty', axis=alt.Axis(labels=False))\n",
    "    ).properties(\n",
    "        width=350,\n",
    "        height=200,\n",
    "        title='Histogram'\n",
    "    ).configure_axis(\n",
    "        labelAngle=45\n",
    "    ).configure_title(\n",
    "        fontSize=14\n",
    "    ).interactive(False)  # Disable interactive features\n",
    "\n",
    "    var_spec_hist_graph_json = hist_chart.to_json()\n",
    "\n",
    "    decil_data = pd.DataFrame({\n",
    "        'Decil': list(decil_var.index),\n",
    "        'Target': list(decil_var.values)\n",
    "    })\n",
    "\n",
    "    # Create the bar plot using Altair\n",
    "    decil_chart = alt.Chart(decil_data).mark_bar().encode(\n",
    "        x=alt.X('Decil', title='Decil', sort=None, axis=alt.Axis(labels=True, labelOverlap=True, labelFontSize=9)),\n",
    "        y=alt.Y('Target', title='% Target', axis=alt.Axis(labels=False))\n",
    "    ).properties(\n",
    "        width=350,\n",
    "        height=200,\n",
    "        title='Target mean per Decil'\n",
    "    ).configure_axis(\n",
    "        labelAngle=45\n",
    "    ).configure_title(\n",
    "        fontSize=12\n",
    "    ).interactive(False)  # Disable interactive features\n",
    "\n",
    "    var_spec_decil_graph_json = decil_chart.to_json()\n",
    "\n",
    "    # Create a dataframe with the number_per_quintile values for each month\n",
    "    df_quintile_time = pd.DataFrame({\n",
    "        'Date': list(variables_espec_time[var_espec].keys()),\n",
    "        'number_per_quintile': [variables_espec_time[var_espec][date]['number_per_quintile'] for date in variables_espec_time[var_espec].keys()]\n",
    "    })\n",
    "\n",
    "    # Expand the number_per_quintile dictionary into separate columns\n",
    "    df_quintile_time = df_quintile_time.join(pd.DataFrame(df_quintile_time.pop('number_per_quintile').tolist(), index=df_quintile_time.index))\n",
    "\n",
    "    # Melt the dataframe to have a long format suitable for Altair\n",
    "    df_melted = df_quintile_time.melt(id_vars='Date', var_name='Quintile', value_name='Count')\n",
    "\n",
    "    # Calculate the percentage for each quintile\n",
    "    df_melted['Percentage'] = df_melted.groupby('Date')['Count'].transform(lambda x: x / x.sum() * 100)\n",
    "\n",
    "    # Create the 100% stacked column chart using Altair\n",
    "    stacked_chart = alt.Chart(df_melted).mark_bar().encode(\n",
    "        x=alt.X('Date', title='Date'),\n",
    "        y=alt.Y('Percentage', title='Percentage', stack='normalize'),\n",
    "        color=alt.Color('Quintile', title='Quintile')\n",
    "    ).properties(\n",
    "        width=800,\n",
    "        height=200,\n",
    "        title='100% Stacked Column Chart'\n",
    "    ).configure_axis(\n",
    "        labelAngle=45\n",
    "    ).configure_title(\n",
    "        fontSize=14\n",
    "    ).configure_legend(\n",
    "        orient='top'\n",
    "    ).interactive(False)  # Disable interactive features\n",
    "\n",
    "    var_spec_stacked_graph_json = stacked_chart.to_json()\n",
    "\n",
    "    var_espec_content[var_espec] = {\n",
    "        \"tab_desc\": var_spec_tab_desc,\n",
    "        \"tab_quant\": var_spec_tab_quant,\n",
    "        \"hist\": var_spec_hist_graph_json,\n",
    "        \"decil\": var_spec_decil_graph_json,\n",
    "        \"hist_time\": var_spec_stacked_graph_json,\n",
    "    }\n",
    "\n",
    "### JINJA TEMPLATE\n",
    "\n",
    "# Read the template from the template sheet\n",
    "with open(\"./template/sandEda_template.html\", \"r\") as file:\n",
    "    sanEda_template = file.read()\n",
    "    \n",
    "# Create a Jinja2 template object\n",
    "\n",
    "template = Template(sanEda_template)\n",
    "\n",
    "# Render the template with the data\n",
    "rendered_html = template.render(\n",
    "    overview_tab_general=overview_tab_general,\n",
    "    overview_tab_full=overview_tab_full,\n",
    "    overview_target_metric=overview_target_metric,\n",
    "    overview_target_name=overview_target_name,\n",
    "    overview_tgt_graph_json=overview_tgt_graph_json,\n",
    "    var_tab_iv=var_tab_iv,\n",
    "    var_tab_mi=var_tab_mi,\n",
    "    var_tab_miss=var_tab_miss,\n",
    "    var_tab_zero=var_tab_zero,\n",
    "    var_tab_psi=var_tab_psi,\n",
    "    var_tab_ks=var_tab_ks,\n",
    "    specific_variables=var_espec_content,\n",
    ")\n",
    "\n",
    "# Save the rendered HTML to a file\n",
    "with open(\"sanEda_report.html\", \"w\") as file:\n",
    "    file.write(rendered_html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "time_name = \"safra\"\n",
    "target_name = \"Exited\"\n",
    "id_name = \"CustomerId\"\n",
    "\n",
    "res_espect_var = {}\n",
    "quintil_cut_points = {}\n",
    "\n",
    "df_filtered = df.drop(\n",
    "    columns=[time_name, target_name, id_name]\n",
    ")\n",
    "\n",
    "for var in df_filtered:\n",
    "    res_espect_time = {}\n",
    "    for time in df[time_name].unique():\n",
    "        df_time = df[df[time_name] == time]\n",
    "\n",
    "        res_espec = {}\n",
    "\n",
    "        if df_time[var].dtypes == \"object\":\n",
    "            le = LabelEncoder()\n",
    "            df_time_enc[var] = le.fit_transform(df_time[var])\n",
    "\n",
    "            quintil_cut_points[var] = (\n",
    "                df[var].quantile([0.2, 0.4, 0.6, 0.8]).to_dict()\n",
    "            )\n",
    "\n",
    "            res_espec = {\n",
    "                \"number_of_missing\": int(df_time[var].isnull().sum()),\n",
    "                \"number_of_zeros\": int((df_time[var] == 0).sum()),\n",
    "                \"sum\": float(df_time[var].sum()),\n",
    "                \"mean\": float(round(df_time[var].mean(), 2)),\n",
    "                \"median\": float(df_time[var].median()),\n",
    "                \"number_per_quintile\": {\n",
    "                    f\"<= {round(quintil_cut_points[var][0.2], 0)}\": int(\n",
    "                        (df_time[var] <= quintil_cut_points[var][0.2]).sum()\n",
    "                    ),\n",
    "                    f\"<= {round(quintil_cut_points[var][0.4], 0)}\": int(\n",
    "                        (\n",
    "                            (df_time[var] > quintil_cut_points[var][0.2])\n",
    "                            & (df_time[var] <= quintil_cut_points[var][0.4])\n",
    "                        ).sum()\n",
    "                    ),\n",
    "                    f\"<= {round(quintil_cut_points[var][0.6], 0)}\": int(\n",
    "                        (\n",
    "                            (df_time[var] > quintil_cut_points[var][0.4])\n",
    "                            & (df_time[var] <= quintil_cut_points[var][0.6])\n",
    "                        ).sum()\n",
    "                    ),\n",
    "                    f\"<= {round(quintil_cut_points[var][0.8], 0)}\": int(\n",
    "                        (\n",
    "                            (df_time[var] > quintil_cut_points[var][0.6])\n",
    "                            & (df_time[var] <= quintil_cut_points[var][0.8])\n",
    "                        ).sum()\n",
    "                    ),\n",
    "                    f\"> {round(quintil_cut_points[var][0.8], 0)}\": int(\n",
    "                        (df_time[var] > quintil_cut_points[var][0.8]).sum()\n",
    "                    ),\n",
    "                },\n",
    "            }\n",
    "        elif df_time[var].dtype in [\"int64\", \"float64\"]:\n",
    "            quintil_cut_points[var] = (\n",
    "                df[var].quantile([0.2, 0.4, 0.6, 0.8]).to_dict()\n",
    "            )\n",
    "            res_espec = {\n",
    "                \"number_of_missing\": int(df_time[var].isnull().sum()),\n",
    "                \"number_of_zeros\": int((df_time[var] == 0).sum()),\n",
    "                \"sum\": float(df_time[var].sum()),\n",
    "                \"mean\": float(round(df_time[var].mean(), 2)),\n",
    "                \"median\": float(df_time[var].median()),\n",
    "                \"number_per_quintile\": {\n",
    "                    f\"<= {round(quintil_cut_points[var][0.2], 0)}\": int(\n",
    "                        (df_time[var] <= quintil_cut_points[var][0.2]).sum()\n",
    "                    ),\n",
    "                    f\"<= {round(quintil_cut_points[var][0.4], 0)}\": int(\n",
    "                        (\n",
    "                            (df_time[var] > quintil_cut_points[var][0.2])\n",
    "                            & (df_time[var] <= quintil_cut_points[var][0.4])\n",
    "                        ).sum()\n",
    "                    ),\n",
    "                    f\"<= {round(quintil_cut_points[var][0.6], 0)}\": int(\n",
    "                        (\n",
    "                            (df_time[var] > quintil_cut_points[var][0.4])\n",
    "                            & (df_time[var] <= quintil_cut_points[var][0.6])\n",
    "                        ).sum()\n",
    "                    ),\n",
    "                    f\"<= {round(quintil_cut_points[var][0.8], 0)}\": int(\n",
    "                        (\n",
    "                            (df_time[var] > quintil_cut_points[var][0.6])\n",
    "                            & (df_time[var] <= quintil_cut_points[var][0.8])\n",
    "                        ).sum()\n",
    "                    ),\n",
    "                    f\"> {round(quintil_cut_points[var][0.8], 0)}\": int(\n",
    "                        (df_time[var] > quintil_cut_points[var][0.8]).sum()\n",
    "                    ),\n",
    "                },\n",
    "            }\n",
    "        res_espect_time[time] = res_espec\n",
    "    res_espect_var[var] = res_espect_time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
